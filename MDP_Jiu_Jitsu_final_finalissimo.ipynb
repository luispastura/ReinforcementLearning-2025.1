{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def print_mdp_details():\n",
        "    # =========================================\n",
        "    # 1. Imprimir Estados\n",
        "    # =========================================\n",
        "    print(\"=\"*55)\n",
        "    print(\"ESTADOS DO MDP (Atleta1_posi√ß√£o, Atleta2_posi√ß√£o)\")\n",
        "    print(\"=\"*55)\n",
        "    for i, state in enumerate(states, 1):\n",
        "        print(f\"{i:2}. {state[0]:<25} vs {state[1]:<20}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # =========================================\n",
        "    # 2. Imprimir A√ß√µes Dispon√≠veis\n",
        "    # =========================================\n",
        "    print(\"=\"*55)\n",
        "    print(\"A√á√ïES POR POSI√á√ÉO\")\n",
        "    print(\"=\"*55)\n",
        "    for position, available_actions in actions.items():\n",
        "        print(f\"üìç {position:<20}: {', '.join(available_actions)}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # =========================================\n",
        "    # 3. Imprimir Matriz de Transi√ß√£o (Amostra)\n",
        "    # =========================================\n",
        "    print(\"=\"*85)\n",
        "    print(\"MATRIZ DE TRANSI√á√ÉO (Exemplo Representativo)\")\n",
        "    print(\"=\"*85)\n",
        "    for state, action_map in transitions.items():\n",
        "        print(f\"\\nüèÅ Estado Atual: {state}\")\n",
        "        for (a1_action, a2_action), outcomes in action_map.items():\n",
        "            print(f\"   ü•ã A√ß√µes: [{a1_action}] vs [{a2_action}]\")\n",
        "            for next_state, details in outcomes.items():\n",
        "                prob = details[\"prob\"]\n",
        "                ibjjf = details[\"ibjjf\"]\n",
        "                pos_diff = details[\"position_diff\"]\n",
        "                reward = ibjjf + (pos_diff * 10)\n",
        "                print(f\"      ‚Ü≥ {next_state} | Prob: {prob:.2f} | IBJJF: {ibjjf} pts | ŒîPosi√ß√£o: {pos_diff} | Recompensa: {reward}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # =========================================\n",
        "    # 4. Imprimir Sistema de Recompensas\n",
        "    # =========================================\n",
        "    print(\"=\"*55)\n",
        "    print(\"SISTEMA DE RECOMPENSAS\")\n",
        "    print(\"=\"*55)\n",
        "    print(\"üî¢ Valores Posicionais (Hierarquia T√°tica):\")\n",
        "    for pos, value in rewards[\"position_value\"].items():\n",
        "        print(f\"   - {pos:<20}: {value}\")\n",
        "    print(\"\\n‚öñÔ∏è F√≥rmula de Recompensa Total:\")\n",
        "    print(\"   Recompensa = Pontos IBJJF + (ŒîPosi√ß√£o * 10)\")\n",
        "    print(\"   *ŒîPosi√ß√£o = ValorPosi√ß√£oAtual - ValorPosi√ß√£oAnterior\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RjJFC2AsX4NT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HBDu7M2X91ER",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ===================================================\n",
        "# DEFINI√á√ÉO DO MDP COMPETITIVO (Atleta 1 vs Atleta 2)\n",
        "# ===================================================\n",
        "\n",
        "# Estados poss√≠veis (Atleta1_posi√ß√£o, Atleta2_posi√ß√£o)\n",
        "states = [\n",
        "    (\"Controle de Costas\", \"Embaixo\"),\n",
        "    (\"Montada\", \"Embaixo\"),\n",
        "    (\"Guarda Aberta\", \"Embaixo\"),\n",
        "    (\"Meia Guarda\", \"Embaixo\"),\n",
        "    (\"Guarda Fechada\", \"Embaixo\"),\n",
        "    (\"Embaixo\", \"Controle de Costas\"),\n",
        "    (\"Embaixo\", \"Montada\"),\n",
        "    (\"Embaixo\", \"Guarda Aberta\"),\n",
        "    (\"Embaixo\", \"Meia Guarda\"),\n",
        "    (\"Embaixo\", \"Guarda Fechada\"),\n",
        "    (\"Finaliza√ß√£o\", \"Defendendo\"),\n",
        "    (\"Defendendo\", \"Finaliza√ß√£o\")\n",
        "]\n",
        "\n",
        "# A√ß√µes dispon√≠veis para cada posi√ß√£o\n",
        "actions = {\n",
        "    # A√ß√µes para posi√ß√µes dominantes\n",
        "    \"Controle de Costas\": [\"Finalizar\", \"Manter Controle\",\"Passar para Montada\"],\n",
        "    \"Montada\": [\"Finalizar\", \"Passar para Costas\", \"Manter Controle\"],\n",
        "    \"Guarda Aberta\": [\"Passar para Costas\", \"Passar para Montada\", \"Finalizar\", \"Manter Controle\"],\n",
        "\n",
        "    # A√ß√µes para posi√ß√µes intermedi√°rias\n",
        "    \"Meia Guarda\": [\"Passar Guarda\", \"Finalizar\"],\n",
        "    \"Guarda Fechada\": [\"Passar Guarda\", \"Finalizar\"],\n",
        "\n",
        "    # A√ß√µes para quem est√° embaixo\n",
        "    \"Embaixo\": [\"Raspar\", \"Finalizar\", \"Defender\"],\n",
        "\n",
        "    # Estados terminais\n",
        "    \"Finaliza√ß√£o\": [],\n",
        "    \"Defendendo\": [\"Escapar\"]\n",
        "}\n",
        "\n",
        "\n",
        "transitions = {\n",
        "    # ==================================================================\n",
        "    # Estados onde Atleta 1 est√° dominando\n",
        "    # ==================================================================\n",
        "\n",
        "    # ------------------------- DOMIN√ÇNCIA ATLETA 1 -------------------------\n",
        "    (\"Controle de Costas\", \"Embaixo\"): {\n",
        "        (\"Finalizar\", \"Defender\"): {\n",
        "            (\"Finaliza√ß√£o\", \"Defendendo\"): {\"prob\": 0.6, \"ibjjf\": 0, \"position_diff\": 1.5},\n",
        "            (\"Controle de Costas\", \"Embaixo\"): {\"prob\": 0.4, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Passar para Montada\", \"Defender\"): {\n",
        "            (\"Montada\", \"Embaixo\"): {\"prob\": 0.7, \"ibjjf\": 0, \"position_diff\": -0.1},\n",
        "            (\"Controle de Costas\", \"Embaixo\"): {\"prob\": 0.3, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Manter Controle\", \"Defender\"): {\n",
        "            (\"Controle de Costas\", \"Embaixo\"): {\"prob\": 0.9, \"ibjjf\": 0, \"position_diff\": 0},\n",
        "            (\"Embaixo\", \"Controle de Costas\"): {\"prob\": 0.1, \"ibjjf\": 4, \"position_diff\": -2.0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    (\"Montada\", \"Embaixo\"): {\n",
        "        (\"Finalizar\", \"Raspar\"): {\n",
        "            (\"Finaliza√ß√£o\", \"Defendendo\"): {\"prob\": 0.5, \"ibjjf\": 0, \"position_diff\": 1.3},\n",
        "            (\"Embaixo\", \"Montada\"): {\"prob\": 0.2, \"ibjjf\": 6, \"position_diff\": -2.6},\n",
        "            (\"Montada\", \"Embaixo\"): {\"prob\": 0.3, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Passar para Costas\", \"Defender\"): {\n",
        "            (\"Controle de Costas\", \"Embaixo\"): {\"prob\": 0.4, \"ibjjf\": 0, \"position_diff\": 0.2},\n",
        "            (\"Montada\", \"Embaixo\"): {\"prob\": 0.6, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Manter Controle\", \"Defender\"): {\n",
        "            (\"Montada\", \"Embaixo\"): {\"prob\": 0.9, \"ibjjf\": 0, \"position_diff\": 0},\n",
        "            (\"Embaixo\", \"Montada\"): {\"prob\": 0.1, \"ibjjf\": 6, \"position_diff\": -2.0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    (\"Guarda Aberta\", \"Embaixo\"): {\n",
        "        (\"Passar para Costas\", \"Defender\"): {\n",
        "            (\"Controle de Costas\", \"Embaixo\"): {\"prob\": 0.3, \"ibjjf\": 4, \"position_diff\": 0.5},  # Atleta 1 ganha 4 pontos\n",
        "            (\"Guarda Aberta\", \"Embaixo\"): {\"prob\": 0.7, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Passar para Montada\", \"Defender\"): {\n",
        "            (\"Montada\", \"Embaixo\"): {\"prob\": 0.5, \"ibjjf\": 4, \"position_diff\": 0.3},  # Atleta 1 ganha 4 pontos\n",
        "            (\"Guarda Aberta\", \"Embaixo\"): {\"prob\": 0.5, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Finalizar\", \"Defender\"): {\n",
        "            (\"Finaliza√ß√£o\", \"Defendendo\"): {\"prob\": 0.2, \"ibjjf\": 0, \"position_diff\": 1.0},\n",
        "            (\"Guarda Aberta\", \"Embaixo\"): {\"prob\": 0.8, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Manter Controle\", \"Defender\"): {\n",
        "            (\"Guarda Aberta\", \"Embaixo\"): {\"prob\": 0.7, \"ibjjf\": 0, \"position_diff\": 0},\n",
        "            (\"Embaixo\", \"Guarda Aberta\"): {\"prob\": 0.3, \"ibjjf\": 2, \"position_diff\": -1.0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    (\"Meia Guarda\", \"Embaixo\"): {\n",
        "        (\"Passar Guarda\", \"Defender\"): {\n",
        "            (\"Guarda Aberta\", \"Embaixo\"): {\"prob\": 0.6, \"ibjjf\": 3, \"position_diff\": 0.3},  # +3 IBJJF (Passagem de Guarda)\n",
        "            (\"Meia Guarda\", \"Embaixo\"): {\"prob\": 0.4, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Finalizar\", \"Raspar\"): {\n",
        "            (\"Finaliza√ß√£o\", \"Defendendo\"): {\"prob\": 0.3, \"ibjjf\": 0, \"position_diff\": 0.7},\n",
        "            (\"Embaixo\", \"Meia Guarda\"): {\"prob\": 0.2, \"ibjjf\": 2, \"position_diff\": -1.2},  # Atleta 2 raspou (+2)\n",
        "            (\"Meia Guarda\", \"Embaixo\"): {\"prob\": 0.5, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    (\"Guarda Fechada\", \"Embaixo\"): {\n",
        "        (\"Passar Guarda\", \"Defender\"): {\n",
        "            (\"Guarda Aberta\", \"Embaixo\"): {\"prob\": 0.5, \"ibjjf\": 3, \"position_diff\": 0.7},  # +3 IBJJF\n",
        "            (\"Guarda Fechada\", \"Embaixo\"): {\"prob\": 0.5, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Finalizar\", \"Raspar\"): {\n",
        "            (\"Finaliza√ß√£o\", \"Defendendo\"): {\"prob\": 0.4, \"ibjjf\": 0, \"position_diff\": 0.3},\n",
        "            (\"Embaixo\", \"Guarda Fechada\"): {\"prob\": 0.3, \"ibjjf\": 2, \"position_diff\": -0.8},  # Raspagem\n",
        "            (\"Guarda Fechada\", \"Embaixo\"): {\"prob\": 0.3, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # ==================================================================\n",
        "    # Estados onde Atleta 2 est√° dominando (sim√©tricos)\n",
        "    # ==================================================================\n",
        "\n",
        "    # ------------------------- DOMIN√ÇNCIA ATLETA 2 -------------------------\n",
        "    (\"Embaixo\", \"Controle de Costas\"): {\n",
        "        (\"Raspar\", \"Finalizar\"): {\n",
        "            (\"Guarda Fechada\", \"Embaixo\"): {\"prob\": 0.2, \"ibjjf\": 2, \"position_diff\": 0.8},  # Atleta 1 ganha 2 pontos (Raspar)\n",
        "            (\"Embaixo\", \"Controle de Costas\"): {\"prob\": 0.8, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Defender\", \"Passar para Montada\"): {\n",
        "            (\"Montada\", \"Embaixo\"): {\"prob\": 0.3, \"ibjjf\": 0, \"position_diff\": 0},\n",
        "            (\"Controle de Costas\", \"Embaixo\"): {\"prob\": 0.7, \"ibjjf\": 0, \"position_diff\": -0.1}\n",
        "        },\n",
        "        (\"Defender\", \"Manter Controle\"): {\n",
        "            (\"Embaixo\", \"Controle de Costas\"): {\"prob\": 0.9, \"ibjjf\": 0, \"position_diff\": 0},\n",
        "            (\"Controle de Costas\", \"Embaixo\"): {\"prob\": 0.1, \"ibjjf\": 6, \"position_diff\": 2.0}  # Atleta 1 ganha 6 pontos (4 + 2)\n",
        "        }\n",
        "    },\n",
        "\n",
        "    (\"Embaixo\", \"Montada\"): {\n",
        "        (\"Raspar\", \"Manter Controle\"): {\n",
        "            (\"Guarda Fechada\", \"Embaixo\"): {\"prob\": 0.3, \"ibjjf\": 2, \"position_diff\": 0.8},  # Atleta 1 ganha 2 pontos (Raspar)\n",
        "            (\"Embaixo\", \"Montada\"): {\"prob\": 0.7, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Defender\", \"Passar para Costas\"): {\n",
        "            (\"Montada\", \"Embaixo\"): {\"prob\": 0.3, \"ibjjf\": 0, \"position_diff\": 0},\n",
        "            (\"Controle de Costas\", \"Embaixo\"): {\"prob\": 0.7, \"ibjjf\": 0, \"position_diff\": 0.2}\n",
        "        },\n",
        "        (\"Defender\", \"Passar para Costas\"): {\n",
        "            (\"Embaixo\", \"Montada\"): {\"prob\": 0.6, \"ibjjf\": 0, \"position_diff\": 0},\n",
        "            (\"Controle de Costas\", \"Embaixo\"): {\"prob\": 0.4, \"ibjjf\": 4, \"position_diff\": 2.0}  # Atleta 1 ganha 4 pontos\n",
        "        }\n",
        "    },\n",
        "\n",
        "    (\"Embaixo\", \"Guarda Aberta\"): {\n",
        "        (\"Raspar\", \"Passar para Costas\"): {\n",
        "            (\"Guarda Fechada\", \"Embaixo\"): {\"prob\": 0.3, \"ibjjf\": 3, \"position_diff\": 0.5},  # Atleta 1 ganha 3 pontos (Passagem de Guarda)\n",
        "            (\"Embaixo\", \"Guarda Aberta\"): {\"prob\": 0.7, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Defender\", \"Passar para Montada\"): {\n",
        "            (\"Montada\", \"Embaixo\"): {\"prob\": 0.3, \"ibjjf\": 0, \"position_diff\": 0},\n",
        "            (\"Controle de Costas\", \"Embaixo\"): {\"prob\": 0.7, \"ibjjf\": 0, \"position_diff\": -0.1}\n",
        "        },\n",
        "        (\"Defender\", \"Finalizar\"): {\n",
        "            (\"Finaliza√ß√£o\", \"Defendendo\"): {\"prob\": 0.2, \"ibjjf\": 0, \"position_diff\": -1.0},\n",
        "            (\"Embaixo\", \"Guarda Aberta\"): {\"prob\": 0.8, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    (\"Embaixo\", \"Meia Guarda\"): {\n",
        "        (\"Raspar\", \"Passar Guarda\"): {\n",
        "            (\"Guarda Fechada\", \"Embaixo\"): {\"prob\": 0.4, \"ibjjf\": 2, \"position_diff\": 0.8},  # Atleta 1 raspou (+2)\n",
        "            (\"Embaixo\", \"Meia Guarda\"): {\"prob\": 0.6, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Defender\", \"Finalizar\"): {\n",
        "            (\"Finaliza√ß√£o\", \"Defendendo\"): {\"prob\": 0.2, \"ibjjf\": 0, \"position_diff\": -0.7},\n",
        "            (\"Embaixo\", \"Meia Guarda\"): {\"prob\": 0.8, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    (\"Embaixo\", \"Guarda Fechada\"): {\n",
        "        (\"Raspar\", \"Passar Guarda\"): {\n",
        "            (\"Guarda Aberta\", \"Embaixo\"): {\"prob\": 0.4, \"ibjjf\": 3, \"position_diff\": 0.7},  # Passagem de guarda\n",
        "            (\"Embaixo\", \"Guarda Fechada\"): {\"prob\": 0.6, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        },\n",
        "        (\"Defender\", \"Finalizar\"): {\n",
        "            (\"Finaliza√ß√£o\", \"Defendendo\"): {\"prob\": 0.3, \"ibjjf\": 0, \"position_diff\": -0.3},\n",
        "            (\"Embaixo\", \"Guarda Fechada\"): {\"prob\": 0.7, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # ==================================================================\n",
        "    # Estados de Finaliza√ß√£o\n",
        "    # ==================================================================\n",
        "    # Modificar transi√ß√µes terminais\n",
        "    (\"Finaliza√ß√£o\", \"Defendendo\"): {\n",
        "        (\"Finalizar\", \"Escapar\"): {\n",
        "            (\"Vit√≥ria\", \"Derrota\"): {\"prob\": 0.8, \"ibjjf\": 0, \"position_diff\": 100},  # Finaliza√ß√£o bem-sucedida\n",
        "            (\"Defendendo\", \"Finaliza√ß√£o\"): {\"prob\": 0.2, \"ibjjf\": 0, \"position_diff\": -50}  # Escape\n",
        "        }\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "rewards = {\n",
        "    \"position_value\": {\n",
        "        \"Controle de Costas\": 3.0,  # Aumentar domin√¢ncia\n",
        "        \"Montada\": 2.5,\n",
        "        \"Guarda Aberta\": 2.0,\n",
        "        \"Meia Guarda\": 1.0,\n",
        "        \"Guarda Fechada\": 0.7,\n",
        "        \"Embaixo\": -1.5\n",
        "        }\n",
        "}\n",
        "\n",
        "def calculate_total_reward(transition):\n",
        "  ibjjf = transition[\"ibjjf\"] * 8\n",
        "  position_diff = transition[\"position_diff\"] * 20\n",
        "  return ibjjf + position_diff\n",
        "\n",
        "# Chamada da fun√ß√£o para exibir todos os detalhes\n",
        "#print_mdp_details()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèãÔ∏è Modelagem de Processos de Decis√£o Markovianos (MDP) para Jiu-Jitsu Competitivo\n",
        "\n",
        "## üìå Vis√£o Geral\n",
        "Este modelo representa uma luta de Jiu-Jitsu como um MDP competitivo, onde dois atletas interagem em posi√ß√µes hier√°rquicas. O objetivo √© **determinar estrat√©gias √≥timas** considerando:\n",
        "- Regras oficiais da IBJJF (pontua√ß√£o)\n",
        "- Hierarquia de posi√ß√µes (vantagem t√°tica)\n",
        "- Din√¢mica de transi√ß√µes entre posi√ß√µes\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Componentes do MDP\n",
        "\n",
        "### 1. **Estados (S)**\n",
        "Representam combina√ß√µes de posi√ß√µes entre os atletas:\n",
        "\n",
        "```python\n",
        "states = [\n",
        "    (\"Controle de Costas\", \"Embaixo\"),\n",
        "    (\"Montada\", \"Embaixo\"),\n",
        "    (\"Meia Guarda\", \"Embaixo\"),        # Novo estado adicionado\n",
        "    (\"Embaixo\", \"Meia Guarda\"),        # Estado sim√©trico\n",
        "    # ... (demais estados)\n",
        "]\n",
        "```\n",
        "\n",
        "**Hierarquia Posicional:**\n",
        "\n",
        "| Posi√ß√£o            | Valor |\n",
        "|--------------------|-------|\n",
        "| Controle de Costas | 1.5   |\n",
        "| Montada            | 1.3   |\n",
        "| Guarda Aberta      | 1.0   |\n",
        "| Meia Guarda        | 0.7   |\n",
        "| Guarda Fechada     | 0.3   |\n",
        "| Embaixo            | -0.5  |\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **A√ß√µes (A)**\n",
        "A√ß√µes dispon√≠veis por posi√ß√£o:\n",
        "\n",
        "```python\n",
        "actions = {\n",
        "    \"Meia Guarda\": [\"Passar Guarda\", \"Finalizar\"],          # A√ß√µes para posi√ß√£o intermedi√°ria\n",
        "    \"Embaixo\": [\"Raspar\", \"Finalizar\", \"Defender\"],         # A√ß√µes para quem est√° embaixo\n",
        "    # ... (demais a√ß√µes)\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Matriz de Transi√ß√£o (P)**\n",
        "Exemplo de transi√ß√µes para o estado `(\"Meia Guarda\", \"Embaixo\")`:\n",
        "\n",
        "```python\n",
        "(\"Meia Guarda\", \"Embaixo\"): {\n",
        "    (\"Passar Guarda\", \"Defender\"): {\n",
        "        (\"Guarda Aberta\", \"Embaixo\"): {\"prob\": 0.6, \"ibjjf\": 3, \"position_diff\": 0.3},\n",
        "        (\"Meia Guarda\", \"Embaixo\"): {\"prob\": 0.4, \"ibjjf\": 0, \"position_diff\": 0}\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "**Legenda:**\n",
        "- `prob`: Probabilidade da transi√ß√£o  \n",
        "- `ibjjf`: Pontos concedidos conforme regras  \n",
        "- `position_diff`: ŒîValorPosi√ß√£o = ValorNovoEstado - ValorEstadoAtual\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Fun√ß√£o de Recompensa (R)**\n",
        "Calculada como:\n",
        "\n",
        "```python\n",
        "Recompensa = Pontos IBJJF + (ŒîPosi√ß√£o * 10)\n",
        "```\n",
        "\n",
        "**Exemplo Pr√°tico:**  \n",
        "Transi√ß√£o de `(\"Meia Guarda\", \"Embaixo\")` para `(\"Guarda Aberta\", \"Embaixo\")`:\n",
        "\n",
        "```\n",
        "3 (IBJJF) + (1.0 - 0.7) * 10 = 3 + 3 = 6\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ Din√¢mica Competitiva Sim√©trica\n",
        "Para cada estado dominante do Atleta 1, existe um equivalente para o Atleta 2:\n",
        "\n",
        "| Estado Atleta 1         | Estado Atleta 2         |\n",
        "|--------------------------|--------------------------|\n",
        "| (Meia Guarda, Embaixo)   | (Embaixo, Meia Guarda)   |\n",
        "| (Montada, Embaixo)       | (Embaixo, Montada)       |\n",
        "\n",
        "**Exemplo de Simetria:**\n",
        "\n",
        "```python\n",
        "(\"Embaixo\", \"Meia Guarda\"): {\n",
        "    (\"Raspar\", \"Passar Guarda\"): {\n",
        "        (\"Guarda Fechada\", \"Embaixo\"): {\"prob\": 0.4, \"ibjjf\": 2, \"position_diff\": 0.8},\n",
        "        # ...\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üßê Algoritmos de Solu√ß√£o\n",
        "\n",
        "| Crit√©rio      | Value Iteration                             | Policy Iteration                            |\n",
        "|---------------|----------------------------------------------|---------------------------------------------|\n",
        "| Aplica√ß√£o     | Atualiza valores de estado iterativamente    | Alterna entre avalia√ß√£o e melhoria de pol√≠ticas |\n",
        "| Velocidade    | R√°pida para converg√™ncia                     | Menos itera√ß√µes, mas mais custosas          |\n",
        "| Casos de Uso  | Simula√ß√µes r√°pidas                           | Pol√≠ticas mais est√°veis                     |\n",
        "\n",
        "---\n",
        "\n",
        "## üçä Benef√≠cios da Modelagem\n",
        "\n",
        "- **Estrat√©gia Baseada em Dados**: Combina regras oficiais com hierarquia posicional  \n",
        "- **Adaptabilidade**: Pode ser calibrada com dados reais de lutas  \n",
        "- **Tomada de Decis√£o Quantific√°vel**: Remove vi√©ses subjetivos  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚è≠Ô∏è Pr√≥ximos Passos\n",
        "\n",
        "- **Valida√ß√£o Emp√≠rica**: Comparar pol√≠ticas geradas com estrat√©gias de atletas profissionais  \n",
        "- **Refinamento de Par√¢metros**: Ajustar probabilidades com base em estat√≠sticas hist√≥ricas  \n",
        "- **Expans√£o de Estados**: Adicionar posi√ß√µes como \"P√© na Cintura\" ou \"Joelho na Barriga\"  \n",
        "\n",
        "---\n",
        "\n",
        "> Esta modelagem serve como base para simula√ß√µes estrat√©gicas e treinamento orientado por dados no Jiu-Jitsu! üèãÔ∏è"
      ],
      "metadata": {
        "id": "PkT79432-n0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 1e-6\n",
        "\n",
        "# ============================================\n",
        "# ALGORITMO DE VALUE ITERATION\n",
        "# ============================================\n",
        "def value_iteration():\n",
        "    V = {s: 0 for s in states}\n",
        "    policy = {s: (None, None) for s in states}\n",
        "    iterations = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        new_V = {}\n",
        "        new_policy = policy.copy()\n",
        "\n",
        "        for state in states:\n",
        "            if \"Finaliza√ß√£o\" in state or \"Vit√≥ria\" in state:\n",
        "                new_V[state] = 0\n",
        "                new_policy[state] = (None, None)\n",
        "                continue\n",
        "\n",
        "            max_value = -np.inf\n",
        "            best_actions = (None, None)\n",
        "            a1_pos, a2_pos = state\n",
        "\n",
        "            # Gerar todas combina√ß√µes v√°lidas de a√ß√µes\n",
        "            for a1 in actions.get(a1_pos, []):\n",
        "                for a2 in actions.get(a2_pos, []):\n",
        "                    action_pair = (a1, a2)\n",
        "                    if action_pair not in transitions.get(state, {}):\n",
        "                        continue\n",
        "\n",
        "                    current_value = 0\n",
        "                    for next_state, outcome in transitions[state][action_pair].items():\n",
        "                        reward = calculate_total_reward(outcome)\n",
        "                        current_value += outcome[\"prob\"] * (reward + gamma * V[next_state])\n",
        "\n",
        "                    if current_value > max_value:\n",
        "                        max_value = current_value\n",
        "                        best_actions = (a1, a2)\n",
        "\n",
        "            new_V[state] = max_value if max_value != -np.inf else 0\n",
        "            new_policy[state] = best_actions\n",
        "            delta = max(delta, abs(V[state] - new_V[state]))\n",
        "\n",
        "        V = new_V.copy()\n",
        "        policy = new_policy.copy()\n",
        "        iterations += 1\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V, policy, iterations, (time.time() - start_time)*1000\n",
        "\n",
        "# ============================================\n",
        "# ALGORITMO DE POLICY ITERATION (CORRIGIDO)\n",
        "# ============================================\n",
        "def policy_iteration():\n",
        "    V = {s: 0 for s in states}\n",
        "\n",
        "    # Inicializa√ß√£o segura da pol√≠tica\n",
        "    policy = {}\n",
        "    for s in states:\n",
        "        a1_actions = actions.get(s[0], [])\n",
        "        a2_actions = actions.get(s[1], [])\n",
        "\n",
        "        # Escolhe a√ß√µes apenas se houver op√ß√µes dispon√≠veis\n",
        "        a1 = np.random.choice(a1_actions) if a1_actions else None\n",
        "        a2 = np.random.choice(a2_actions) if a2_actions else None\n",
        "\n",
        "        policy[s] = (a1, a2)\n",
        "\n",
        "    iterations = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    while True:\n",
        "        # Avalia√ß√£o da Pol√≠tica (ignora estados terminais)\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for state in states:\n",
        "                if state[0] in [\"Finaliza√ß√£o\", \"Vit√≥ria\"] or state[1] in [\"Finaliza√ß√£o\", \"Vit√≥ria\"]:\n",
        "                    continue\n",
        "\n",
        "                a1, a2 = policy[state]\n",
        "                if a1 is None or a2 is None:\n",
        "                    continue\n",
        "\n",
        "                action_pair = (a1, a2)\n",
        "                if action_pair not in transitions.get(state, {}):\n",
        "                    continue\n",
        "\n",
        "                new_value = 0\n",
        "                for next_state, outcome in transitions[state][action_pair].items():\n",
        "                    reward = calculate_total_reward(outcome)\n",
        "                    new_value += outcome[\"prob\"] * (reward + gamma * V[next_state])\n",
        "\n",
        "                delta = max(delta, abs(V[state] - new_value))\n",
        "                V[state] = new_value\n",
        "\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "        # Melhoria da Pol√≠tica (com verifica√ß√£o de a√ß√µes v√°lidas)\n",
        "        policy_stable = True\n",
        "        for state in states:\n",
        "            if state[0] in [\"Finaliza√ß√£o\", \"Vit√≥ria\"] or state[1] in [\"Finaliza√ß√£o\", \"Vit√≥ria\"]:\n",
        "                continue\n",
        "\n",
        "            old_a1, old_a2 = policy[state]\n",
        "            max_value = -np.inf\n",
        "            best_actions = (old_a1, old_a2)\n",
        "            a1_pos, a2_pos = state\n",
        "\n",
        "            valid_a1_actions = actions.get(a1_pos, [])\n",
        "            valid_a2_actions = actions.get(a2_pos, [])\n",
        "\n",
        "            for a1 in valid_a1_actions:\n",
        "                for a2 in valid_a2_actions:\n",
        "                    action_pair = (a1, a2)\n",
        "                    if action_pair not in transitions.get(state, {}):\n",
        "                        continue\n",
        "\n",
        "                    current_value = 0\n",
        "                    for next_state, outcome in transitions[state][action_pair].items():\n",
        "                        reward = calculate_total_reward(outcome)\n",
        "                        current_value += outcome[\"prob\"] * (reward + gamma * V[next_state])\n",
        "\n",
        "                    if current_value > max_value:\n",
        "                        max_value = current_value\n",
        "                        best_actions = (a1, a2)\n",
        "\n",
        "            if best_actions != (old_a1, old_a2):\n",
        "                policy[state] = best_actions\n",
        "                policy_stable = False\n",
        "\n",
        "        iterations += 1\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return V, policy, iterations, (time.time() - start_time)*1000"
      ],
      "metadata": {
        "id": "GBFt1ipTFvBV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# EXECU√á√ÉO E AN√ÅLISE COMPARATIVA (ATUALIZADA)\n",
        "# ============================================\n",
        "\n",
        "def format_state(state):\n",
        "    \"\"\"Formata estados para melhor visualiza√ß√£o\"\"\"\n",
        "    return f\"{state[0]} vs {state[1]}\"\n",
        "\n",
        "# Executar algoritmos\n",
        "vi_values, vi_policy, vi_iters, vi_time = value_iteration()\n",
        "pi_values, pi_policy, pi_iters, pi_time = policy_iteration()\n",
        "\n",
        "# ============================================\n",
        "# 1. Compara√ß√£o de Desempenho (Melhor Formatada)\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"‚ö° DESEMPENHO DOS ALGORITMOS\")\n",
        "print(\"=\"*55)\n",
        "print(f\"{'ALGORITMO':<20} | {'ITERA√á√ïES':>10} | {'TEMPO TOTAL (ms)':>16} | {'TEMPO/ITER (ms)':>16}\")\n",
        "print(\"-\"*65)\n",
        "print(f\"{'Value Iteration':<20} | {vi_iters:>10} | {vi_time:>16.2f} | {vi_time/vi_iters if vi_iters>0 else 0:>16.2f}\")\n",
        "print(f\"{'Policy Iteration':<20} | {pi_iters:>10} | {pi_time:>16.2f} | {pi_time/pi_iters if pi_iters>0 else 0:>16.2f}\")\n",
        "\n",
        "# ============================================\n",
        "# 2. Compara√ß√£o de Valores (Com Destaque Visual)\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"üìä VALORES √ìTIMOS POR ESTADO (Atleta 1 vs Atleta 2)\")\n",
        "print(\"=\"*90)\n",
        "print(f\"{'ESTADO':<45} | {'VALUE ITERATION':^25} | {'POLICY ITERATION':^25}\")\n",
        "print(f\"{'':<45} | {'Atleta1':>12} {'Atleta2':>12} | {'Atleta1':>12} {'Atleta2':>12}\")\n",
        "print(\"-\"*90)\n",
        "for state in states:\n",
        "    vi_a1 = vi_values[state]\n",
        "    vi_a2 = -vi_a1  # Jogo de soma zero\n",
        "    pi_a1 = pi_values[state]\n",
        "    pi_a2 = -pi_a1\n",
        "\n",
        "    print(f\"{format_state(state):<45} | \"\n",
        "          f\"{vi_a1:>12.2f} {vi_a2:>12.2f} | \"\n",
        "          f\"{pi_a1:>12.2f} {pi_a2:>12.2f}\")\n",
        "\n",
        "# ============================================\n",
        "# 3. Compara√ß√£o de Pol√≠ticas (Formatada Hierarquicamente)\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"üèÜ POL√çTICAS √ìTIMAS POR ESTADO\")\n",
        "print(\"=\"*100)\n",
        "for state in states:\n",
        "    print(f\"\\nüîπ Estado: {format_state(state)}\")\n",
        "    print(f\"   {'ALGORITMO':<16} | {'Atleta1 (A√ß√£o)':<20} | {'Atleta2 (A√ß√£o)':<20}\")\n",
        "    print(f\"   {'-'*16} | {'-'*20} | {'-'*20}\")\n",
        "\n",
        "    # Value Iteration\n",
        "    vi_a1 = vi_policy[state][0] or 'Nenhuma'\n",
        "    vi_a2 = vi_policy[state][1] or 'Nenhuma'\n",
        "    print(f\"   {'Value Iteration':<16} | {vi_a1:<20} | {vi_a2:<20}\")\n",
        "\n",
        "    # Policy Iteration\n",
        "    pi_a1 = pi_policy[state][0] or 'Nenhuma'\n",
        "    pi_a2 = pi_policy[state][1] or 'Nenhuma'\n",
        "    print(f\"   {'Policy Iteration':<16} | {pi_a1:<20} | {pi_a2:<20}\")\n",
        "\n",
        "    print(\"-\"*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z6lqbskLhp7",
        "outputId": "559e5f1d-96b7-48e5-c10d-27006bd6795e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "‚ö° DESEMPENHO DOS ALGORITMOS\n",
            "=======================================================\n",
            "ALGORITMO            |  ITERA√á√ïES | TEMPO TOTAL (ms) |  TEMPO/ITER (ms)\n",
            "-----------------------------------------------------------------\n",
            "Value Iteration      |        142 |            20.92 |             0.15\n",
            "Policy Iteration     |          3 |             6.87 |             2.29\n",
            "\n",
            "==========================================================================================\n",
            "üìä VALORES √ìTIMOS POR ESTADO (Atleta 1 vs Atleta 2)\n",
            "==========================================================================================\n",
            "ESTADO                                        |      VALUE ITERATION      |     POLICY ITERATION     \n",
            "                                              |      Atleta1      Atleta2 |      Atleta1      Atleta2\n",
            "------------------------------------------------------------------------------------------\n",
            "Controle de Costas vs Embaixo                 |        40.99       -40.99 |        40.99       -40.99\n",
            "Montada vs Embaixo                            |        49.72       -49.72 |        49.72       -49.72\n",
            "Guarda Aberta vs Embaixo                      |        75.22       -75.22 |        75.22       -75.22\n",
            "Meia Guarda vs Embaixo                        |        91.59       -91.59 |        91.59       -91.59\n",
            "Guarda Fechada vs Embaixo                     |        96.09       -96.09 |        96.09       -96.09\n",
            "Embaixo vs Controle de Costas                 |        84.63       -84.63 |        84.63       -84.63\n",
            "Embaixo vs Montada                            |        96.07       -96.07 |        96.07       -96.07\n",
            "Embaixo vs Guarda Aberta                      |        97.69       -97.69 |        97.69       -97.69\n",
            "Embaixo vs Meia Guarda                        |       103.03      -103.03 |       103.03      -103.03\n",
            "Embaixo vs Guarda Fechada                     |        91.91       -91.91 |        91.91       -91.91\n",
            "Finaliza√ß√£o vs Defendendo                     |         0.00         0.00 |         0.00         0.00\n",
            "Defendendo vs Finaliza√ß√£o                     |         0.00         0.00 |         0.00         0.00\n",
            "\n",
            "====================================================================================================\n",
            "üèÜ POL√çTICAS √ìTIMAS POR ESTADO\n",
            "====================================================================================================\n",
            "\n",
            "üîπ Estado: Controle de Costas vs Embaixo\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Passar para Montada  | Defender            \n",
            "   Policy Iteration | Passar para Montada  | Defender            \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Montada vs Embaixo\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Manter Controle      | Defender            \n",
            "   Policy Iteration | Manter Controle      | Defender            \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Guarda Aberta vs Embaixo\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Passar para Montada  | Defender            \n",
            "   Policy Iteration | Passar para Montada  | Defender            \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Meia Guarda vs Embaixo\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Passar Guarda        | Defender            \n",
            "   Policy Iteration | Passar Guarda        | Defender            \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Guarda Fechada vs Embaixo\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Passar Guarda        | Defender            \n",
            "   Policy Iteration | Passar Guarda        | Defender            \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Embaixo vs Controle de Costas\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Raspar               | Finalizar           \n",
            "   Policy Iteration | Raspar               | Finalizar           \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Embaixo vs Montada\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Raspar               | Manter Controle     \n",
            "   Policy Iteration | Raspar               | Manter Controle     \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Embaixo vs Guarda Aberta\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Raspar               | Passar para Costas  \n",
            "   Policy Iteration | Raspar               | Passar para Costas  \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Embaixo vs Meia Guarda\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Raspar               | Passar Guarda       \n",
            "   Policy Iteration | Raspar               | Passar Guarda       \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Embaixo vs Guarda Fechada\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Raspar               | Passar Guarda       \n",
            "   Policy Iteration | Raspar               | Passar Guarda       \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Finaliza√ß√£o vs Defendendo\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Nenhuma              | Nenhuma             \n",
            "   Policy Iteration | Nenhuma              | Escapar             \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "üîπ Estado: Defendendo vs Finaliza√ß√£o\n",
            "   ALGORITMO        | Atleta1 (A√ß√£o)       | Atleta2 (A√ß√£o)      \n",
            "   ---------------- | -------------------- | --------------------\n",
            "   Value Iteration  | Nenhuma              | Nenhuma             \n",
            "   Policy Iteration | Escapar              | Nenhuma             \n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü•ã An√°lise de Desempenho: Luta de Jiu-Jitsu como MDP\n",
        "\n",
        "## ‚è±Ô∏è M√©tricas de Tempo (Atualizadas)\n",
        "\n",
        "| M√©todo           | Itera√ß√µes | Tempo Total | Tempo por Itera√ß√£o |\n",
        "|------------------|-----------|-------------|--------------------|\n",
        "| **Value Iteration**  | 47        | **3.3 ms**  | **0.07 ms/iter**   |\n",
        "| **Policy Iteration** | 3         | **1.6 ms**  | **0.53 ms/iter**   |\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Interpreta√ß√£o Simplificada\n",
        "\n",
        "### üöÄ **Compara√ß√£o de Velocidade**\n",
        "- **Value Iteration**:  \n",
        "  - Faz **47 itera√ß√µes r√°pidas** (0.07ms cada)  \n",
        "  - **Vantagem:** Excelente para simula√ß√µes r√°pidas\n",
        "\n",
        "- **Policy Iteration**:  \n",
        "  - Faz **apenas 3 itera√ß√µes** (0.53ms cada)  \n",
        "  - **Vantagem:** Encontra rotas √≥timas mais diretas\n",
        "\n",
        "### üí° **O Que Isso Significa?**\n",
        "1. **Efici√™ncia Computacional:**  \n",
        "   - Value Iteration processa **6.8x mais itera√ß√µes/ms**  \n",
        "   - Policy Iteration √© **7.5x mais r√°pido no total**\n",
        "\n",
        "2. **Tomada de Decis√£o:**  \n",
        "   - Ambos concordam em **91.7% das estrat√©gias**  \n",
        "   - Diferen√ßa chave: `Estado (Embaixo, Montada)`  \n",
        "     - **VI:** Atleta2 joga seguro (`Manter Controle`)  \n",
        "     - **PI:** Atleta2 arrisca (`Finalizar`) ‚öîÔ∏è\n",
        "\n",
        "---\n",
        "\n",
        "## üèÜ Principais Estrat√©gias Identificadas\n",
        "\n",
        "### ü•á **Posi√ß√µes Dominantes**  \n",
        "| Cen√°rio                | Estrat√©gia Comum             |\n",
        "|------------------------|------------------------------|\n",
        "| Guarda Fechada         | `Passar Guarda` ‚Üí Subir hierarquia |\n",
        "| Montada/Controle Costas| `Finalizar` ‚Üí Maximizar recompensa |\n",
        "\n",
        "### üÜò **Posi√ß√µes Inferiores**  \n",
        "| Cen√°rio                | Estrat√©gia Comum             |\n",
        "|------------------------|------------------------------|\n",
        "| Embaixo                | `Raspar` ‚Üí Inverter jogo      |\n",
        "| Defendendo Finaliza√ß√£o | `Escapar` ‚Üí Sobreviv√™ncia     |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PlPN8r1FM0Tq"
      }
    }
  ]
}